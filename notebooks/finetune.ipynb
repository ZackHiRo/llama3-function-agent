{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü¶ô Fine-Tuning Llama 3 8B for Function Calling\n",
        "\n",
        "This notebook demonstrates how to fine-tune **Llama 3 8B Instruct** using **QLoRA** (Quantized Low-Rank Adaptation) with the **Unsloth** library for efficient training on Google Colab T4 GPUs.\n",
        "\n",
        "## üìã Overview\n",
        "\n",
        "- **Base Model**: `unsloth/llama-3-8b-Instruct-bnb-4bit`\n",
        "- **Dataset**: `glaiveai/glaive-function-calling-v2`\n",
        "- **Method**: QLoRA (4-bit quantization + LoRA adapters)\n",
        "- **Hardware**: Google Colab T4 GPU (16GB VRAM)\n",
        "- **Output**: GGUF format for Ollama deployment\n",
        "\n",
        "## üéØ What You'll Learn\n",
        "\n",
        "1. Setting up Unsloth for efficient fine-tuning\n",
        "2. Loading and preparing function calling datasets\n",
        "3. Formatting data for Llama 3 ChatML template\n",
        "4. Configuring and running QLoRA training\n",
        "5. Exporting models to GGUF format for inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üì¶ Step 1: Install Dependencies\n",
        "\n",
        "First, we install the required libraries:\n",
        "\n",
        "- **unsloth**: Optimized library for fast LLM fine-tuning (2x faster, 50% less memory)\n",
        "- **xformers**: Memory-efficient attention mechanisms\n",
        "- **trl**: Transformer Reinforcement Learning library (includes SFTTrainer)\n",
        "- **peft**: Parameter-Efficient Fine-Tuning library\n",
        "\n",
        "> ‚ö†Ô∏è **Note**: Run this cell first and restart the runtime if prompted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install Unsloth with Colab-specific optimizations\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# Install xformers for memory-efficient attention\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "\n",
        "# Install additional dependencies\n",
        "!pip install datasets huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify installation and check GPU\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîß Step 2: Load the Pre-trained Model\n",
        "\n",
        "We use Unsloth's optimized 4-bit quantized version of Llama 3 8B Instruct. This model:\n",
        "\n",
        "- Uses **4-bit NormalFloat (NF4)** quantization for memory efficiency\n",
        "- Is pre-optimized for fast inference and training\n",
        "- Includes all Llama 3 instruction-following capabilities\n",
        "\n",
        "### LoRA Configuration\n",
        "\n",
        "We configure LoRA (Low-Rank Adaptation) with:\n",
        "- **Rank (r)**: 16 - balance between capacity and efficiency\n",
        "- **Alpha**: 16 - scaling factor for LoRA weights\n",
        "- **Target Modules**: All linear layers for comprehensive adaptation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
        "MAX_SEQ_LENGTH = 2048  # Maximum sequence length for training\n",
        "DTYPE = None  # Auto-detect (float16 for T4, bfloat16 for A100)\n",
        "LOAD_IN_4BIT = True  # Use 4-bit quantization\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=DTYPE,\n",
        "    load_in_4bit=LOAD_IN_4BIT,\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {MODEL_NAME}\")\n",
        "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank - higher = more capacity, more memory\n",
        "    target_modules=[\n",
        "        \"q_proj\",   # Query projection\n",
        "        \"k_proj\",   # Key projection\n",
        "        \"v_proj\",   # Value projection\n",
        "        \"o_proj\",   # Output projection\n",
        "        \"gate_proj\",  # MLP gate\n",
        "        \"up_proj\",    # MLP up\n",
        "        \"down_proj\",  # MLP down\n",
        "    ],\n",
        "    lora_alpha=16,  # LoRA scaling factor\n",
        "    lora_dropout=0,  # No dropout for efficiency (Unsloth optimized)\n",
        "    bias=\"none\",  # No bias terms\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Memory optimization\n",
        "    random_state=42,\n",
        "    use_rslora=False,  # Rank-Stabilized LoRA (optional)\n",
        "    loftq_config=None,  # LoftQ initialization (optional)\n",
        ")\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nTrainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "print(f\"Total parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Step 3: Load and Explore the Dataset\n",
        "\n",
        "We use the **Glaive Function Calling v2** dataset, which contains:\n",
        "\n",
        "- **113K+ examples** of function calling conversations\n",
        "- **System prompts** with function definitions\n",
        "- **User queries** with natural language requests\n",
        "- **Assistant responses** with proper function calls and results\n",
        "\n",
        "This dataset is ideal for training models to:\n",
        "1. Understand when to call functions\n",
        "2. Generate properly formatted function calls\n",
        "3. Process function results and respond naturally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the Glaive Function Calling dataset\n",
        "DATASET_NAME = \"glaiveai/glaive-function-calling-v2\"\n",
        "\n",
        "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
        "\n",
        "print(f\"Dataset: {DATASET_NAME}\")\n",
        "print(f\"Number of examples: {len(dataset):,}\")\n",
        "print(f\"\\nColumns: {dataset.column_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore a sample from the dataset\n",
        "sample = dataset[0]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SAMPLE DATA STRUCTURE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for key, value in sample.items():\n",
        "    print(f\"\\nüìå {key.upper()}:\")\n",
        "    print(\"-\" * 40)\n",
        "    # Truncate long values for display\n",
        "    display_value = str(value)[:500] + \"...\" if len(str(value)) > 500 else str(value)\n",
        "    print(display_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîÑ Step 4: Data Formatting for Llama 3 ChatML\n",
        "\n",
        "Llama 3 uses a specific chat template format called **ChatML**. We need to convert the dataset into this format:\n",
        "\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{assistant_message}<|eot_id|>\n",
        "```\n",
        "\n",
        "### Special Tokens\n",
        "\n",
        "| Token | Purpose |\n",
        "|-------|--------|\n",
        "| `<\\|begin_of_text\\|>` | Start of the conversation |\n",
        "| `<\\|start_header_id\\|>` | Start of a role header |\n",
        "| `<\\|end_header_id\\|>` | End of a role header |\n",
        "| `<\\|eot_id\\|>` | End of turn marker |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "\n",
        "def parse_chat_messages(chat_string: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Parse the raw chat string from the dataset into structured messages.\n",
        "    \n",
        "    The dataset format uses markers like:\n",
        "    - SYSTEM: ... \n",
        "    - USER: ...\n",
        "    - ASSISTANT: ...\n",
        "    - FUNCTION RESPONSE: ...\n",
        "    \n",
        "    Args:\n",
        "        chat_string: Raw chat string from the dataset\n",
        "        \n",
        "    Returns:\n",
        "        List of message dictionaries with 'role' and 'content' keys\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    \n",
        "    # Pattern to match role markers\n",
        "    pattern = r'(SYSTEM|USER|ASSISTANT|FUNCTION RESPONSE):\\s*'\n",
        "    \n",
        "    # Split by role markers while keeping the markers\n",
        "    parts = re.split(pattern, chat_string)\n",
        "    \n",
        "    # Process parts pairwise (role, content)\n",
        "    i = 1  # Skip the first empty part\n",
        "    while i < len(parts) - 1:\n",
        "        role = parts[i].strip().lower()\n",
        "        content = parts[i + 1].strip()\n",
        "        \n",
        "        # Map roles to standard format\n",
        "        role_map = {\n",
        "            'system': 'system',\n",
        "            'user': 'user',\n",
        "            'assistant': 'assistant',\n",
        "            'function response': 'function_response'\n",
        "        }\n",
        "        \n",
        "        mapped_role = role_map.get(role, role)\n",
        "        \n",
        "        if content:  # Only add non-empty messages\n",
        "            messages.append({\n",
        "                'role': mapped_role,\n",
        "                'content': content\n",
        "            })\n",
        "        \n",
        "        i += 2\n",
        "    \n",
        "    return messages\n",
        "\n",
        "\n",
        "def format_to_llama3_chatml(messages: List[Dict[str, str]]) -> str:\n",
        "    \"\"\"\n",
        "    Convert structured messages to Llama 3 ChatML format.\n",
        "    \n",
        "    Args:\n",
        "        messages: List of message dicts with 'role' and 'content'\n",
        "        \n",
        "    Returns:\n",
        "        Formatted string in Llama 3 ChatML format\n",
        "    \"\"\"\n",
        "    formatted_parts = [\"<|begin_of_text|>\"]\n",
        "    \n",
        "    for msg in messages:\n",
        "        role = msg['role']\n",
        "        content = msg['content']\n",
        "        \n",
        "        # Handle function responses as part of assistant turn\n",
        "        if role == 'function_response':\n",
        "            # Append function response to previous assistant message or create new\n",
        "            formatted_parts.append(\n",
        "                f\"<|start_header_id|>function<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
        "            )\n",
        "        else:\n",
        "            formatted_parts.append(\n",
        "                f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
        "            )\n",
        "    \n",
        "    return \"\".join(formatted_parts)\n",
        "\n",
        "\n",
        "def clean_system_prompt(system_prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean the system prompt by removing redundant role prefixes.\n",
        "    \n",
        "    The dataset's 'system' field often starts with 'SYSTEM: ' which is\n",
        "    redundant since we're already placing it in the system role.\n",
        "    \n",
        "    Args:\n",
        "        system_prompt: Raw system prompt from dataset\n",
        "        \n",
        "    Returns:\n",
        "        Cleaned system prompt without role prefix\n",
        "    \"\"\"\n",
        "    # Remove \"SYSTEM: \" prefix if present (case-insensitive)\n",
        "    cleaned = re.sub(r'^SYSTEM:\\s*', '', system_prompt, flags=re.IGNORECASE)\n",
        "    return cleaned.strip()\n",
        "\n",
        "\n",
        "def format_dataset_example(example: Dict[str, Any]) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Format a single dataset example into Llama 3 ChatML format.\n",
        "    \n",
        "    This is the main formatting function used for dataset mapping.\n",
        "    \n",
        "    Args:\n",
        "        example: Raw dataset example with 'system', 'chat' columns\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with 'text' key containing formatted conversation\n",
        "    \"\"\"\n",
        "    # Get system prompt and chat content\n",
        "    system_prompt = example.get('system', '')\n",
        "    chat_content = example.get('chat', '')\n",
        "    \n",
        "    # Clean the system prompt (remove \"SYSTEM: \" prefix)\n",
        "    system_prompt = clean_system_prompt(system_prompt)\n",
        "    \n",
        "    # Parse the chat into structured messages\n",
        "    messages = parse_chat_messages(chat_content)\n",
        "    \n",
        "    # Add system message at the beginning if present\n",
        "    if system_prompt:\n",
        "        messages.insert(0, {'role': 'system', 'content': system_prompt})\n",
        "    \n",
        "    # Format to Llama 3 ChatML\n",
        "    formatted_text = format_to_llama3_chatml(messages)\n",
        "    \n",
        "    return {'text': formatted_text}\n",
        "\n",
        "\n",
        "# Test the formatting function\n",
        "print(\"Testing format function on sample...\")\n",
        "print(\"=\" * 60)\n",
        "test_result = format_dataset_example(dataset[0])\n",
        "print(test_result['text'][:1000])\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply formatting to the entire dataset\n",
        "print(\"Formatting dataset...\")\n",
        "\n",
        "formatted_dataset = dataset.map(\n",
        "    format_dataset_example,\n",
        "    remove_columns=dataset.column_names,  # Remove original columns\n",
        "    desc=\"Formatting to Llama 3 ChatML\",\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Formatted {len(formatted_dataset):,} examples\")\n",
        "print(f\"Columns: {formatted_dataset.column_names}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FORMATTED SAMPLE:\")\n",
        "print(\"=\" * 60)\n",
        "print(formatted_dataset[0]['text'][:800])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üèãÔ∏è Step 5: Configure and Run Training\n",
        "\n",
        "We use the **SFTTrainer** (Supervised Fine-Tuning Trainer) from the TRL library with the following hyperparameters:\n",
        "\n",
        "| Parameter | Value | Description |\n",
        "|-----------|-------|-------------|\n",
        "| Learning Rate | 2e-4 | Standard for QLoRA fine-tuning |\n",
        "| Batch Size | 2 | Per-device batch size (T4 compatible) |\n",
        "| Gradient Accumulation | 4 | Effective batch size = 8 |\n",
        "| Max Steps | 60 | Number of training steps |\n",
        "| Warmup Steps | 5 | Learning rate warmup |\n",
        "| Optimizer | AdamW 8-bit | Memory-efficient optimizer |\n",
        "\n",
        "> üí° **Tip**: For production training, increase `max_steps` to 500-1000 or use `num_train_epochs`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Training configuration\n",
        "OUTPUT_DIR = \"./llama3-function-calling-lora\"\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 2\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "MAX_STEPS = 60\n",
        "WARMUP_STEPS = 5\n",
        "LOGGING_STEPS = 10\n",
        "SAVE_STEPS = 30\n",
        "\n",
        "# Configure training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=not is_bfloat16_supported(),  # Use FP16 on T4\n",
        "    bf16=is_bfloat16_supported(),  # Use BF16 on A100/H100\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    optim=\"adamw_8bit\",  # Memory-efficient optimizer\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=42,\n",
        "    report_to=\"none\",  # Disable W&B/MLflow logging\n",
        ")\n",
        "\n",
        "print(\"Training configuration:\")\n",
        "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  - Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  - Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  - Max steps: {MAX_STEPS}\")\n",
        "print(f\"  - Using BF16: {is_bfloat16_supported()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=formatted_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dataset_num_proc=2,  # Parallel data processing\n",
        "    packing=False,  # Disable packing for function calling data\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ SFTTrainer initialized\")\n",
        "print(f\"Training examples: {len(formatted_dataset):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show GPU memory before training\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "reserved_memory = torch.cuda.memory_reserved() / 1e9\n",
        "max_memory = gpu_stats.total_memory / 1e9\n",
        "\n",
        "print(f\"GPU Memory before training:\")\n",
        "print(f\"  - Reserved: {reserved_memory:.2f} GB\")\n",
        "print(f\"  - Total: {max_memory:.2f} GB\")\n",
        "print(f\"  - Available: {max_memory - reserved_memory:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ Start training!\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Training complete!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display training statistics\n",
        "print(\"\\nüìä Training Statistics:\")\n",
        "print(f\"  - Total steps: {trainer_stats.global_step}\")\n",
        "print(f\"  - Training loss: {trainer_stats.training_loss:.4f}\")\n",
        "print(f\"  - Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
        "print(f\"  - Samples/second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")\n",
        "\n",
        "# Show final GPU memory usage\n",
        "used_memory = torch.cuda.max_memory_reserved() / 1e9\n",
        "print(f\"\\nüíæ Peak GPU Memory: {used_memory:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß™ Step 6: Test the Fine-Tuned Model\n",
        "\n",
        "Let's verify the model works correctly by running inference on a test prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Test prompt\n",
        "test_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a helpful AI assistant that can perform function calls.\n",
        "When asked to perform actions, respond with a JSON object containing:\n",
        "- \"action\": the action to perform\n",
        "- \"parameters\": an object with relevant parameters\n",
        "- \"reasoning\": brief explanation of your approach\n",
        "\n",
        "Available functions:\n",
        "- get_weather(location: str, units: str = \"metric\")\n",
        "- search_web(query: str, num_results: int = 5)\n",
        "- send_email(to: str, subject: str, body: str)<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "What's the weather like in Tokyo right now?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate response\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "# Decode and display\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL RESPONSE:\")\n",
        "print(\"=\" * 60)\n",
        "# Extract just the assistant's response\n",
        "assistant_response = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n",
        "print(assistant_response.split(\"<|eot_id|>\")[0].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üíæ Step 7: Save the Model\n",
        "\n",
        "We'll save the model in multiple formats:\n",
        "\n",
        "1. **LoRA Adapters**: Lightweight adapter weights only\n",
        "2. **Merged Model**: Full model with adapters merged\n",
        "3. **GGUF Format**: Quantized format for Ollama/llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save LoRA adapters (lightweight, ~50MB)\n",
        "LORA_OUTPUT_DIR = \"./llama3-function-calling-lora\"\n",
        "\n",
        "model.save_pretrained(LORA_OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(LORA_OUTPUT_DIR)\n",
        "\n",
        "print(f\"‚úÖ LoRA adapters saved to: {LORA_OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save merged model (full 16-bit model, ~16GB)\n",
        "# Uncomment if you have enough disk space\n",
        "\n",
        "# MERGED_OUTPUT_DIR = \"./llama3-function-calling-merged\"\n",
        "# model.save_pretrained_merged(\n",
        "#     MERGED_OUTPUT_DIR,\n",
        "#     tokenizer,\n",
        "#     save_method=\"merged_16bit\",\n",
        "# )\n",
        "# print(f\"‚úÖ Merged model saved to: {MERGED_OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üì¶ Step 8: Export to GGUF Format\n",
        "\n",
        "GGUF (GPT-Generated Unified Format) is the standard format for:\n",
        "- **Ollama**: Local LLM deployment\n",
        "- **llama.cpp**: CPU/GPU inference\n",
        "- **LM Studio**: Desktop LLM application\n",
        "\n",
        "### Quantization Options\n",
        "\n",
        "| Method | Size | Quality | Use Case |\n",
        "|--------|------|---------|----------|\n",
        "| `q8_0` | ~8GB | Highest | Production, when memory allows |\n",
        "| `q4_k_m` | ~4.5GB | Good | Balanced quality/size |\n",
        "| `q4_0` | ~4GB | Acceptable | Memory-constrained |\n",
        "\n",
        "> üí° **Recommendation**: Use `q4_k_m` for the best balance of quality and size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export to GGUF format (for Ollama)\n",
        "GGUF_OUTPUT_DIR = \"./llama3-function-calling-gguf\"\n",
        "QUANTIZATION_METHOD = \"q4_k_m\"  # Options: q8_0, q4_k_m, q5_k_m, q4_0, f16\n",
        "\n",
        "print(f\"Exporting to GGUF format with {QUANTIZATION_METHOD} quantization...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "model.save_pretrained_gguf(\n",
        "    GGUF_OUTPUT_DIR,\n",
        "    tokenizer,\n",
        "    quantization_method=QUANTIZATION_METHOD,\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ GGUF model saved to: {GGUF_OUTPUT_DIR}\")\n",
        "print(f\"Quantization: {QUANTIZATION_METHOD}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List the exported files\n",
        "import os\n",
        "\n",
        "print(\"\\nüìÅ Exported files:\")\n",
        "for root, dirs, files in os.walk(GGUF_OUTPUT_DIR):\n",
        "    for file in files:\n",
        "        filepath = os.path.join(root, file)\n",
        "        size_mb = os.path.getsize(filepath) / 1e6\n",
        "        print(f\"  - {file}: {size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üöÄ Step 9: Push to Hugging Face Hub\n",
        "\n",
        "Share your fine-tuned model with the community by uploading it to the Hugging Face Hub.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "1. Create a [Hugging Face account](https://huggingface.co/join)\n",
        "2. Create a new model repository\n",
        "3. Generate an access token with write permissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Login to Hugging Face (run this cell and enter your token)\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Option 1: Interactive login (will prompt for token)\n",
        "login()\n",
        "\n",
        "# Option 2: Use token directly (uncomment and replace with your token)\n",
        "# login(token=\"hf_your_token_here\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Push LoRA adapters to Hub\n",
        "HF_USERNAME = \"your-username\"  # Replace with your Hugging Face username\n",
        "MODEL_NAME = \"llama3-8b-function-calling-lora\"\n",
        "\n",
        "# Push the LoRA model\n",
        "model.push_to_hub(\n",
        "    f\"{HF_USERNAME}/{MODEL_NAME}\",\n",
        "    tokenizer=tokenizer,\n",
        "    private=False,  # Set to True for private models\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Model pushed to: https://huggingface.co/{HF_USERNAME}/{MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Push GGUF to Hub (optional)\n",
        "GGUF_REPO_NAME = \"llama3-8b-function-calling-gguf\"\n",
        "\n",
        "model.push_to_hub_gguf(\n",
        "    f\"{HF_USERNAME}/{GGUF_REPO_NAME}\",\n",
        "    tokenizer=tokenizer,\n",
        "    quantization_method=QUANTIZATION_METHOD,\n",
        "    private=False,\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ GGUF model pushed to: https://huggingface.co/{HF_USERNAME}/{GGUF_REPO_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîß Step 10: Deploy with Ollama\n",
        "\n",
        "Once you have the GGUF file, you can deploy it with Ollama:\n",
        "\n",
        "### Create Modelfile\n",
        "\n",
        "```dockerfile\n",
        "# Modelfile\n",
        "FROM ./llama3-function-calling-gguf/unsloth.Q4_K_M.gguf\n",
        "\n",
        "PARAMETER temperature 0.7\n",
        "PARAMETER num_ctx 4096\n",
        "PARAMETER stop \"<|eot_id|>\"\n",
        "\n",
        "TEMPLATE \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{{ .System }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{{ .Prompt }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM \"\"\"You are a helpful AI assistant that can perform function calls.\n",
        "When asked to perform actions, respond with a JSON object containing:\n",
        "- \"action\": the action to perform\n",
        "- \"parameters\": an object with relevant parameters\n",
        "- \"reasoning\": brief explanation of your approach\"\"\"\n",
        "```\n",
        "\n",
        "### Create and Run\n",
        "\n",
        "```bash\n",
        "# Create the model in Ollama\n",
        "ollama create llama3-function-calling -f Modelfile\n",
        "\n",
        "# Run the model\n",
        "ollama run llama3-function-calling\n",
        "\n",
        "# Test with API\n",
        "curl http://localhost:11434/api/generate -d '{\n",
        "  \"model\": \"llama3-function-calling\",\n",
        "  \"prompt\": \"Get the weather in Tokyo\"\n",
        "}'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã Summary\n",
        "\n",
        "In this notebook, we:\n",
        "\n",
        "1. ‚úÖ Installed Unsloth and dependencies for efficient QLoRA training\n",
        "2. ‚úÖ Loaded Llama 3 8B Instruct with 4-bit quantization\n",
        "3. ‚úÖ Configured LoRA adapters for parameter-efficient fine-tuning\n",
        "4. ‚úÖ Loaded and formatted the Glaive function calling dataset\n",
        "5. ‚úÖ Implemented Llama 3 ChatML formatting\n",
        "6. ‚úÖ Trained the model with SFTTrainer\n",
        "7. ‚úÖ Exported to GGUF format for Ollama deployment\n",
        "8. ‚úÖ Pushed to Hugging Face Hub\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- üìà Increase training steps for better performance\n",
        "- üî¨ Experiment with different LoRA ranks and alpha values\n",
        "- üìä Add evaluation metrics and validation\n",
        "- üéØ Fine-tune on domain-specific function calling data\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Unsloth Documentation](https://github.com/unslothai/unsloth)\n",
        "- [Llama 3 Model Card](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n",
        "- [TRL Library](https://github.com/huggingface/trl)\n",
        "- [Ollama Documentation](https://ollama.ai/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up GPU memory\n",
        "import gc\n",
        "\n",
        "del model\n",
        "del tokenizer\n",
        "del trainer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"‚úÖ Cleanup complete!\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
